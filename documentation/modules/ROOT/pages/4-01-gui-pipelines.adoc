= 4.1 Automating workflows with data science pipelines

include::_attributes.adoc[]

In previous sections of this {deliverable}, you used a notebook to train and save your data model. Optionally, you can automate these tasks by using {productname-long} pipelines. Pipelines offer a way to automate the execution of multiple notebooks and python code, and to schedule them to run. By using pipelines, you can execute long training jobs or retrain your models on a schedule without having to manually run them in a notebook.

In this first section, we will create a very simple pipeline. The pipeline will use the notebook that you used in previous sections, to train a model and then save it to S3 storage.

Note: The completed pipeline should look like the one in the `6 Train Save.pipeline` file.

== Creating a pipeline

. Start from you workbench's JupyterLab environment.
+
image::pipelines/wb-pipeline-launcher.png[Pipeline Buttons]

. If the launcher is not visible, click *+* to open it.
+
image::pipelines/wb-launcher-button.png[Launcher Button]

* Click *Pipeline Editor*.
+
image::pipelines/wb-pipeline-editor-button.png[Pipeline Editor Button]

Great.  You've created a blank pipeline! 

== Setting Pipeline Properties

Set some properties for the pipeline and any nodes that are created. Use the default
runtime image for when you run your notebook or python code.
//is there more than one notebook?

. In the pipeline editor, click  *Open Panel* 
+
image::pipelines/wb-pipeline-panel-button.png[Open Panel Button]
+
image::pipelines/wb-pipeline-panel-button-loc.png[Open Panel]

. Select the *Pipeline Properties* tab.
+
image::pipelines/wb-pipeline-properties-tab.png[Pipeline Properties Tab]

. In the  Pipeline Properties panel, scroll down to *Generic Node Defaults* and *Runtime Image*.  Set the value to `Tensorflow with Cuda and Python 3.9 (UBI 9)`
+
image::pipelines/wb-pipeline-runtime-image.png[Pipeline Runtime Image0]

== Creating Nodes

So now that the pipeline is set up.  Let's add some steps, or *nodes* in our pipeline.  Our two nodes will use notebooks `1_experiment_train.ipynb` and `2_save_model.ipynb`.

* Click in the left filebrowser, and drag the notebooks onto the pipeline canvas.

image::pipelines/wb-pipeline-drag-drop.png[ Drag and Drop Notebooks]

* Connect the output port of `1_experiment_train.ipynb` to the input port of `2_save_model.ipynb` by drawing a line from one to another.  Click and drag from the right point on node 1 to the left point on node 2.

image::pipelines/wb-pipeline-connect-nodes.png[Connect Nodes]

== Setting Node Properties

Now that our pipeline has been constructed, we need to set some properties to include and pass files between nodes.  First, let's select node 1, `1_experiment_train.ipynb`.

* Click the node, `1_experiment_train.ipynb` to select it.  It will be outlined in blue.

image::pipelines/wb-pipeline-node-1.png[Select Node 1]

* Click on the *Node Properties* tab in the properties panel.

Now we can add the *File Dependencies* to include the data to train our model.  If we don't set this, the file won't be included in the node when it runs and the training job would fail.

* Scroll down and click *Add*` under *File Dependencies*.

image::pipelines/wb-pipeline-node-1-file-dep.png[Add File Dependency]

* Set the value to `data/card_transdata.csv`.

image::pipelines/wb-pipeline-node-1-file-dep-form.png[Set File Dependency Value]

==  Node Outputs 

In node 1, we'll be creating `models/fraud/model.onnx` and in node 2, we want to upload that file to S3.  We need to set the output of node 1 so that it will be available for node 2.

* Select node 1

* Select the *Node Properties* tab

* Scroll down and click *Add*` under *Output Files*.

image::pipelines/wb-pipeline-node-1-add-output.png[]

* Set the value to `models/fraud/model.onnx`.

image::pipelines/wb-pipeline-node-1-file-output-form.png[Set File Dependency Value]

Now let's set the input of node 2.

* Select node 2

* Select the *Node Properties* tab

* Scroll down and click *Add*` under *Output Files*.

image::pipelines/wb-pipeline-node-1-add-output.png[]

* Set the value to `models/fraud/model.onnx`.

image::pipelines/wb-pipeline-node-1-file-output-form.png[Set File Dependency Value]


== Node Properties for Data Connections

Node 2 will be uploading the model to S3.  We need to set the S3 bucket and keys for the file.  We'll be using the secret created by the Data Connection `My Storage` in the project setup.  The secret is called `aws-connection-my-storage` as you can see by hovering over the connection info *?* in the *Data Connections* tab.  If you called your data connection something other than `My Storage`, you'll need to use look up the secret name.

image::pipelines/dsp-dc-secret-name.png[My Storage Secret Name]

We can use this secret in our pipeline nodes without having to save the information in our pipeline code.  This is important if you would like to save these pipelines to source control without any secret keys.

The secret `aws-connection-my-storage` has the following fields:

```
AWS_ACCESS_KEY_ID
AWS_DEFAULT_REGION
AWS_S3_BUCKET
AWS_S3_ENDPOINT
AWS_SECRET_ACCESS_KEY
```

First let's examine the node properties for node 2.

* Select node 2

* Select the *Node Properties* tab

Here', you'll see some under *Additional Properties* that *Environment Variables* that have been pre-filled out.  The editor inferred you'd need them from the notebook code.  


Since we don't want to save the value in our pipelines, we'll remove all of these environment variables.

* Click *Remove* for each of the environment variables.

image::pipelines/wb-pipeline-node-remove-env-var.png[Remove Env Var]

Now that your node is pipeline is clean, let's add the S3 bucket and keys using the Kubernetes secret.

* Click *Add* under *Kubernetes Secrets*.

image::pipelines/wb-pipeline-add-kube-secret.png[Add Kube Secret]

* Edit the form to set the values for the S3 bucket and keys.

image::pipelines/wb-pipeline-add-kube-secret.png[Add Kube Secret]

* Fill out the form with the following information
** *Environment Variable*: `AWS_ACCESS_KEY_ID`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_ACCESS_KEY_ID`

image::pipelines/wb-pipeline-kube-secret-form.png[Secret Form]

Repeat the procedure for the other keys.

* Add a new secret and fill out the form with the following information.
** *Environment Variable*: `AWS_SECRET_ACCESS_KEY`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_SECRET_ACCESS_KEY`
* Add a new secret and fill out the form with the following information.
** *Environment Variable*: `AWS_S3_ENDPOINT`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_S3_ENDPOINT`
* Add a new secret and fill out the form with the following information.
** *Environment Variable*: `AWS_DEFAULT_REGION`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_DEFAULT_REGION`
* Add a new secret and fill out the form with the following information.
** *Environment Variable*: `AWS_S3_BUCKET`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_S3_BUCKET`

* At this point you can *Save* and *Rename* the `.pipeline` file to save your changes.

== Running the Pipeline 

Upload the pipeline on the cluster itself and run it.  You can do so directly from the pipeline editor.  You can use your own newly created pipeline for this or `6 Train Save.pipeline`.

* Click the play button in the toolbar of the pipeline editor.

image::pipelines/wb-pipeline-run-button.png[Pipeline Run Button]

* Enter a name for your pipeline
* Verify the *Runtime Configuration:* is set to `Data Science Pipeline`.  
* Click *OK*

NOTE: If `Data Science Pipeline` is not available as a runtime configuration, you may have created your notebook before the pipeline server was available.  You can restart your notebook after the pipeline server has been created in your data science project.

image::pipelines/wb-pipeline-run.png[Pipeline Run]

. Return to your data science project and expand the newly created pipeline.
+
image::pipelines/dsp-pipeline-complete.png[]

. Click the pipeline or the pipeline run and then view the pipeline run in progress.
+
image::pipelines/pipeline-run-complete.png[]

The result should be a `models/fraud/model.onnx` file in your S3 bucket which you can serve, just like we manually did in the previous section.


.Next steps

xref:4-02-sdk-pipelines.adoc[Automating workflows with data science pipelines]]